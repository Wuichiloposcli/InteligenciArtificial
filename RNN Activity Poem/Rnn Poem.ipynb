{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias\n"
      ],
      "metadata": {
        "id": "LWTwOJhcdpFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import random"
      ],
      "metadata": {
        "id": "BEwernkPdnp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 1. Cargar y limpiar texto ---\n",
        "Primero abrí el archivo de poemas y realicé una limpieza básica pero muy importante: pasé todo a minúsculas, eliminé tildes sin quitar la letra base, borré todos los números y también quité símbolos raros o especiales que no aportan nada al modelo. Al final, normalicé los espacios para que el texto quedara uniforme y sin ruido innecesario."
      ],
      "metadata": {
        "id": "Ehek1s6kdr1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Pablo Neruda 363 Spanish poems - Dataset .txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', txt)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    txt = re.sub(r\"\\d+\", \"\", txt)               # Elimina todos los números\n",
        "    txt = re.sub(r\"[^a-zñ\\s]\", \"\", txt)         # Elimina cualquier otro caracter raro\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()      # Normaliza espacios\n",
        "    return txt\n",
        "\n",
        "cleaned_text = clean_text(text)"
      ],
      "metadata": {
        "id": "zhY9f1vfd0hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 2. Tokenización a nivel palabra ---\n",
        "Usé Tokenizer de Keras para convertir cada palabra del corpus en un número entero único. Esto es fundamental para que el modelo pueda trabajar con datos numéricos. También obtuve el tamaño total del vocabulario, que es útil para definir la red neuronal más adelante."
      ],
      "metadata": {
        "id": "iTjoG4ETd2Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "tokens = tokenizer.texts_to_sequences([cleaned_text])[0]\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "XeTfqFi3d6mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 3. Generador de secuencias ---\n",
        "Aquí definí una clase que construye secuencias de entrenamiento dinámicamente por lotes. Esto ayuda a no cargar todo en RAM al mismo tiempo, que era el problema que tenía antes. El generador produce pares de entrada y salida, donde la entrada es una secuencia de palabras y la salida es la siguiente palabra esperada"
      ],
      "metadata": {
        "id": "gaPChLrEd8cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NerudaSequenceGenerator(Sequence):\n",
        "    def __init__(self, tokens, seq_length, batch_size, vocab_size):\n",
        "        self.tokens = tokens\n",
        "        self.seq_length = seq_length\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.indexes = range(seq_length, len(tokens))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil((len(self.tokens) - self.seq_length) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X, y = [], []\n",
        "        start = idx * self.batch_size + self.seq_length\n",
        "        end = min(start + self.batch_size, len(self.tokens))\n",
        "\n",
        "        for i in range(start, end):\n",
        "            seq = self.tokens[i - self.seq_length:i]\n",
        "            X.append(seq[:-1])\n",
        "            y.append(seq[-1])\n",
        "\n",
        "        X = pad_sequences(X, maxlen=self.seq_length-1, padding='pre')\n",
        "        y = np.array(y)  # Sparse output (no one-hot)\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "e2yNvktceAuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 4. Crear generador ---\n",
        "Aquí inicializo el generador con secuencias de longitud 20 y un batch size de 64, que me pareció un buen punto medio entre rendimiento y eficiencia de memoria."
      ],
      "metadata": {
        "id": "WxtBSr9HeCUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "generator = NerudaSequenceGenerator(tokens, sequence_length, batch_size, vocab_size)"
      ],
      "metadata": {
        "id": "_g9F3lQjeHwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 5. Crear modelo ---\n",
        "Diseñé un modelo secuencial con una capa de embeddings (para representar semánticamente las palabras), seguida de dos capas LSTM con Dropout entre ellas para evitar overfitting, y finalmente una capa Dense que predice la siguiente palabra. Usé sparse_categorical_crossentropy como función de pérdida para no tener que usar one-hot encoding y ahorrar memoria."
      ],
      "metadata": {
        "id": "u4eRKEJceH_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=sequence_length-1))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "s9lASYnVePmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- 6. Entrenamiento ---\n",
        "Finalmente, entrené el modelo usando el generador. Le puse EarlyStopping por si el modelo deja de mejorar, y ModelCheckpoint para guardar el mejor modelo encontrado. Lo dejé entrenando por 100 épocas porque quiero un resultado de calidad, aunque sé que puedo detenerlo antes si la pérdida se estabiliza."
      ],
      "metadata": {
        "id": "F_9jgCUaeR4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(\"modelo_poemas.h5\", save_best_only=True)\n",
        "\n",
        "model.fit(generator, epochs=100, callbacks=[early_stop, checkpoint])\n"
      ],
      "metadata": {
        "id": "AGEhCMY8U4Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prueba\n",
        "Esta función toma una frase inicial (seed_text) y genera un número determinado de palabras (next_words). La temperatura ajusta la aleatoriedad del modelo: si es baja, el modelo elige las palabras más probables; si es alta, permite mayor variación y creatividad"
      ],
      "metadata": {
        "id": "unsfvwIRessI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poem(seed_text, next_words=30, temperature=1.0):\n",
        "    result = []\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=sequence_length-1, padding='pre')\n",
        "\n",
        "        # Obtener predicciones\n",
        "        preds = model.predict(token_list, verbose=0)[0]\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "\n",
        "        # Aplicar temperatura\n",
        "        preds = np.log(preds + 1e-7) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Elegir palabra siguiente por muestreo probabilístico\n",
        "        predicted_index = np.random.choice(range(len(preds)), p=preds)\n",
        "\n",
        "        # Convertir índice a palabra\n",
        "        output_word = tokenizer.index_word.get(predicted_index, '')\n",
        "        if output_word == '':\n",
        "            continue  # Si no se encuentra, salta esa predicción\n",
        "\n",
        "        seed_text += ' ' + output_word\n",
        "        result.append(output_word)\n",
        "    return ' '.join(result)\n"
      ],
      "metadata": {
        "id": "qblkNZU3U7kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultado"
      ],
      "metadata": {
        "id": "LZBjmoWcevI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"amor\"\n",
        "print(generate_poem(seed, next_words=50, temperature=0.2))"
      ],
      "metadata": {
        "id": "_utvih63U9Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones\n",
        "Este proyecto me permitió entrenar una red neuronal recurrente (LSTM) capaz de generar poesía en español a partir de un corpus extenso de Pablo Neruda. Durante el desarrollo, enfrenté algunos retos de memoria debido a la cantidad de secuencias generadas, pero implementé un generador de datos personalizado que resolvió eficazmente ese problema, permitiendo entrenar el modelo por lotes y sin saturar la RAM.\n",
        "\n",
        "Gracias a la limpieza adecuada del texto (removiendo tildes, símbolos y números), el modelo logró concentrarse en las palabras realmente relevantes. El uso de embeddings ayudó a capturar relaciones semánticas entre palabras, y la arquitectura LSTM con Dropout previno el sobreajuste.\n",
        "\n",
        "Finalmente, implementé una función de generación de texto controlada por temperatura, lo que me permitió ajustar la creatividad de los poemas generados. El modelo logró aprender estilos coherentes y producir versos que, si bien no son perfectos, tienen una estructura y vocabulario similar al original, lo cual valida el entrenamiento."
      ],
      "metadata": {
        "id": "9LFfEgfselGZ"
      }
    }
  ]
}
