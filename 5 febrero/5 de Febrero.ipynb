{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Examen parte 1",
   "id": "6e649c86d48efd49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parte teorica",
   "id": "e4b5792a16d6af58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.\tMention two practical applications of AI in everyday life.\n",
    "Una Inteligencia artificial puede ser cuando desbloqueamos nuestro celular, o usamos la busqueda por voz para obtener información.\n",
    "\n",
    "2. What is the difference between supervised and unsupervised learning?\n",
    "El aprendizaje supervisado es cuando se le dan las etiquetas a la inteligencia artificial para poder aprender mientras que el no supervisado es cuando los datos no han sido clasificados y le toca al algoritmo aprender para poder ir clasificando.\n",
    "\n",
    "3.\tDefine the concept of \"overfitting\" in the context of AI.\n",
    "Cuando se tiene tanta información de un solo tema que hace que la inteligencia artificial se especializa en dichos casos y pierde generalidad de la información, haciendo que solo pueda clasificar o hacer elecciones con base a datos muy especificos.\n",
    "\n",
    "4.\tWhat are some of the rules and approaches that organizations such as Google and the European Union have defined for the development and use of AI\n",
    "Se debe de respetar que en la creación de inteligencia artificial no sea con fines ilegales o que puedan provocar un daño a personas, un ejemplo es que Google no permite la creacion de IA para la guerra.\n",
    "\n",
    "5.\tExplain the importance of normalization in databases.\n",
    "La normalizacion de los datos hace que los procesos sean más eficientes y optimos al momento de trabajar con un modelo, en este caso la IA.\n",
    "\n",
    "6.\tWhat is an imbalanced dataset and how can it affect the performance of a model?\n",
    "Si la informacion no esta balanceada podría provocar un sobreajuste del modelo o podría tener muchos errores el modelo ya que fue entrenado con informacion no proporcional.\n",
    "\n",
    "7.\tExplain the concept of \"dimensionality\" in a dataset. (Open question)\n",
    "Tiene m observaciones y n caracteristicas, cada característica representa una dimensión en un espacio n-dimensional, dondecada punto es en donde se guardan los datos.\n",
    "\n",
    "8.\tWhat does the precision metric measure in a classification model?\n",
    "Mide que tantos positivos verdaderos da nuestro modelo de clasificación, con esto podemos saber que tan confiable es nuestro modelo al calsificar. \n",
    "\n",
    "9.\tWhat is the difference between uninformed search and informed search?\n",
    "En la busqueda sin informacion no se conoce los costos por viajar a un punto mientras que en la informada si existen los costos y se busca que, conociendo dichos costos, encontrar el camino más corto.\n",
    "\n",
    "10.\tName a practical problem where informed search can be applied.\n",
    "Cuando se busca la ruta más optima de un punto a otro, digamos cuando se quiere viajar de un estado a otro usando google maps se elije que camino se quiere tomar. El que tiene la coutas o el camino que no tiene coutas,"
   ],
   "id": "843f0bf3f4d507b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excercises Part 1",
   "id": "271ffc7f0656593e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Identify if the dataset is a Good dataset  or need a technique of preprocessing. Justify the answer\n",
    "Process all the document  of :\n",
    "\n",
    "•\tIris\n",
    "\n",
    "•\tIris1\n",
    "\n",
    "•\tIris2\n",
    "\n",
    "•\tIris3"
   ],
   "id": "60a001b6d92d0338"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Iris",
   "id": "3f57ced4795d9e11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:58:31.673383Z",
     "start_time": "2025-02-06T01:58:31.635290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "iris = pd.read_csv('Data/iris.csv')\n",
    "iris.describe()"
   ],
   "id": "ec35a4ed58b6dae2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "species         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:58:38.771190Z",
     "start_time": "2025-02-06T01:58:38.760771Z"
    }
   },
   "cell_type": "code",
   "source": "iris.isnull().sum()",
   "id": "24332e0003994069",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "species         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Se observa que el DataSet de Iris es bueno ya que no contiene valores desproporcionales y no contiene valores nulos en sus entradas.\n",
    "\n",
    "Ahora se hará con Iris1"
   ],
   "id": "9c3933578ef007d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:59:42.757915Z",
     "start_time": "2025-02-06T01:59:42.685752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iris1 = pd.read_csv('Data/iris1.csv')\n",
    "iris1.describe()"
   ],
   "id": "31ef70280ff731a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       sepal_length  sepal_width  petal_length    petal_width\n",
       "count    150.000000   150.000000    150.000000     150.000000\n",
       "mean     445.763333     3.054000     30.398667    1334.518667\n",
       "std     4919.643273     0.433594    326.354170   16329.850644\n",
       "min        4.300000     2.000000      1.000000       0.100000\n",
       "25%        5.100000     2.800000      1.600000       0.300000\n",
       "50%        5.800000     3.000000      4.350000       1.300000\n",
       "75%        6.400000     3.300000      5.100000       1.800000\n",
       "max    60000.200000     4.400000   4000.700000  200000.200000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>445.763333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>30.398667</td>\n",
       "      <td>1334.518667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4919.643273</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>326.354170</td>\n",
       "      <td>16329.850644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>60000.200000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4000.700000</td>\n",
       "      <td>200000.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T01:59:45.072844Z",
     "start_time": "2025-02-06T01:59:45.063813Z"
    }
   },
   "cell_type": "code",
   "source": "iris1.isnull().sum()",
   "id": "36777465d567002e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "species         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Este dataset necesita una tecnica de preprocesamiento antes de ser utilizado dado que muestra valores atípicos en las caracteristicas y por ello podría influenciar nuestro modelo.\n",
    "\n",
    "Ahora se hará con Iris2"
   ],
   "id": "eee11557ad6e7252"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T02:07:58.615368Z",
     "start_time": "2025-02-06T02:07:58.535096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iris2 = pd.read_csv('Data/iris2.csv')\n",
    "iris2.describe()"
   ],
   "id": "3bddaef8acfa35e2",
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 5 fields in line 12, saw 6\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m iris2 \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mData/iris2.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m iris2\u001B[38;5;241m.\u001B[39mdescribe()\n",
      "File \u001B[1;32m~\\PycharmProjects\\BadeDeDatos\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\BadeDeDatos\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\BadeDeDatos\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\BadeDeDatos\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2061\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 5 fields in line 12, saw 6\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Se observa que no se puede leer el archivo porque tiene un elemento vacio en la linea 12, se podría arreglar eliminando dicho elemento.\n",
    "\n",
    "\n",
    "Ahora se hará con Iris3"
   ],
   "id": "26a952fc157e681f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T02:12:34.609827Z",
     "start_time": "2025-02-06T02:12:34.577825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iris3 = pd.read_csv('Data/iris3.csv')\n",
    "iris3.describe()"
   ],
   "id": "7415beb41bbed4a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T02:12:48.179374Z",
     "start_time": "2025-02-06T02:12:48.169432Z"
    }
   },
   "cell_type": "code",
   "source": "iris3.isnull().sum()",
   "id": "d05e62983b8ed5c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    0\n",
       "sepal_width     0\n",
       "petal_length    0\n",
       "petal_width     0\n",
       "species         5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Se observa que para manjear adecuadamente este dataset es necesario eliminar algunas columnas vacias para poder trabajar sin problemas. De ahi en fuera el dataset se muestra optimo",
   "id": "3c8c252dee396e18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excercise 2 Metrics",
   "id": "6ecb9013fc4b52b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Metrics obtained Example 1Credit Approval\n",
    "\n",
    "•\tAccuracy: 0.88\n",
    "\n",
    "•\tPrecision: 0.82\n",
    "\n",
    "•\tRecall: 0.75\n",
    "\n",
    "•\tF1-Score: 0.78\n",
    "\n",
    "•\tConfusion Matrix:\n",
    "\n",
    "•\tTP[[170  20]FN\n",
    "\n",
    "•\t FP[ 15 105]]TN\n",
    "\n"
   ],
   "id": "dcd9068e19342cea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tiene un desempeño no tan malo pero puede mejorar mucho más ya que la métrica del Recall es baja, ya que un 25% de las aprobaciones reales estan siendo clasificadas de forma incorrecta haciendo que se pierdan muchas personas en el proceso.",
   "id": "dcd13cb6eb5b79f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Example 2 Credit Approval\n",
    "\n",
    "•\tAccuracy: 0.92\n",
    "\n",
    "•\tPrecision: 0.91\n",
    "\n",
    "•\tRecall: 0.93\n",
    "\n",
    "•\tF1-Score: 0.92\n",
    "\n",
    "•\tConfusion Matrix:\n",
    "\n",
    "•\t[[145   5]\n",
    "\n",
    "•\t [  7 143]]\n"
   ],
   "id": "44d6e43f474c9ec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Este modelo es mejor que el anterior, aqui no se pierden tantos clientes al clasificarlos y se observa que el F1-Score es alto, diciendonos que hay un balance optimo entre Recall y precisión.",
   "id": "7e0fc0471ab3dbfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excercise 3 Searchs",
   "id": "12067e0d4c29f85c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T02:30:23.178741Z",
     "start_time": "2025-02-06T02:30:23.167930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import deque\n",
    "\n",
    "def read_graph(file_path):\n",
    "    \"\"\"Lee el archivo y representa el grafo como un diccionario de adyacencia.\"\"\"\n",
    "    graph = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            node1, node2, cost = line.strip().split()\n",
    "            cost = int(cost)\n",
    "            if node1 not in graph:\n",
    "                graph[node1] = []\n",
    "            graph[node1].append((node2, cost))\n",
    "    return graph\n",
    "\n",
    "def bfs(graph, start, goal):\n",
    "    \"\"\"Búsqueda en anchura (BFS)\"\"\"\n",
    "    queue = deque([(start, [start], 0)])\n",
    "    while queue:\n",
    "        node, path, cost = queue.popleft()\n",
    "        if node == goal:\n",
    "            return path, cost  # Retorna el costo acumulado correctamente\n",
    "        for neighbor, edge_cost in graph.get(node, []):\n",
    "            if neighbor not in path:\n",
    "                queue.append((neighbor, path + [neighbor], cost + edge_cost))\n",
    "    return None, float('inf')\n",
    "\n",
    "def dfs(graph, start, goal):\n",
    "    \"\"\"Búsqueda en profundidad (DFS)\"\"\"\n",
    "    stack = [(start, [start], 0)]\n",
    "    while stack:\n",
    "        node, path, cost = stack.pop()\n",
    "        if node == goal:\n",
    "            return path, cost\n",
    "        for neighbor, edge_cost in graph.get(node, []):\n",
    "            if neighbor not in path:\n",
    "                stack.append((neighbor, path + [neighbor], cost + edge_cost))\n",
    "    return None, float('inf')\n",
    "\n",
    "def ucs(graph, start, goal):\n",
    "    \"\"\"Búsqueda de costo uniforme (UCS)\"\"\"\n",
    "    priority_queue = [(0, start, [start])]\n",
    "    visited = {}\n",
    "    while priority_queue:\n",
    "        cost, node, path = heapq.heappop(priority_queue)\n",
    "        if node in visited and visited[node] <= cost:\n",
    "            continue\n",
    "        visited[node] = cost\n",
    "        if node == goal:\n",
    "            return path, cost\n",
    "        for neighbor, edge_cost in graph.get(node, []):\n",
    "            heapq.heappush(priority_queue, (cost + edge_cost, neighbor, path + [neighbor]))\n",
    "    return None, float('inf')\n",
    "\n",
    "# Leer el grafo del archivo\n",
    "graph = read_graph(\"Data/Grafo.txt\")\n",
    "\n",
    "# Definir nodos de inicio y meta\n",
    "start_node = \"A\"\n",
    "goal_node = \"E\"\n",
    "\n",
    "# Ejecutar los algoritmos y mostrar resultados\n",
    "for search_algo, name in [(bfs, \"BFS\"), (dfs, \"DFS\"), (ucs, \"UCS\")]:\n",
    "    path, cost = search_algo(graph, start_node, goal_node)\n",
    "    print(f\"{name} - Path: {path}, Cost: {cost}\")"
   ],
   "id": "75ce8210c4b625fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [('B', 1), ('C', 4)], 'B': [('D', 2)], 'C': [('D', 1)], 'D': [('E', 3)]}\n",
      "BFS - Path: ['A', 'B', 'D', 'E'], Cost: 6\n",
      "DFS - Path: ['A', 'C', 'D', 'E'], Cost: 8\n",
      "UCS - Path: ['A', 'B', 'D', 'E'], Cost: 6\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Excercise 2 \n",
    "\n",
    "A graph representing a map of cities is provided. Each node in the graph is a city, and the edges represent connections between cities with an associated cost (distance in kilometers). The goal is to find the shortest route from a start city to a target city using the A* algorithm.\n",
    "\n",
    "In addition, a table with the straight-line distances (heuristic) from each city to the target city is provided. This heuristic will be used by the A* algorithm to guide the search.\n",
    "\n",
    "Implementation of Algorithm A:*\n",
    "\n",
    "Implement the A* algorithm using the provided heuristic.\n",
    "\n",
    "The algorithm must:\n",
    "\n",
    "Compute the total cost f(n)=g(n)+h(n), where: g(n) is the cumulative cost from the start city to the current city.\n",
    "h(n) is the heuristic (straight-line distance) from the current city to the target city.\n",
    "\n",
    "Explore the cities in order of lowest f(n).\n",
    "\n",
    "Return the optimal route and total cost.\n",
    "Code Requirements:\n",
    "\n",
    "The code must be written in Python.\n",
    "\n",
    "It must include comments explaining each part of the code.\n",
    "\n",
    "It must read the input files (graph and heuristics) and build the necessary data structures.\n",
    "\n",
    "It must print the path found and the total cost.\n"
   ],
   "id": "a484a462035bcc6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T02:54:34.650658Z",
     "start_time": "2025-02-06T02:54:34.627320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "\n",
    "def read_graph(file_path):\n",
    "    graph = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            node1, node2, cost = line.split()\n",
    "            cost = int(cost)\n",
    "            if node1 not in graph:\n",
    "                graph[node1] = []\n",
    "            graph[node1].append((node2, cost))\n",
    "    return graph\n",
    "\n",
    "def read_heuristic(file_path):\n",
    "    heuristic = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            node, value = line.split()\n",
    "            heuristic[node] = int(value)\n",
    "    return heuristic\n",
    "\n",
    "def a_star_graph_search(graph, start, goal, heuristic):\n",
    "    frontier = []\n",
    "    heapq.heappush(frontier, (0 + heuristic[start], start))\n",
    "    came_from = {}\n",
    "    cost_so_far = {start: 0}\n",
    "    visited = set()\n",
    "    \n",
    "    while frontier:\n",
    "        current_cost, current_node = heapq.heappop(frontier)\n",
    "        \n",
    "        if current_node == goal:\n",
    "            # Reconstruir el camino\n",
    "            path = []\n",
    "            total_cost = cost_so_far[current_node]  # El costo real acumulado hasta el nodo objetivo\n",
    "            total_cost_with_heuristic = total_cost + heuristic[goal]  # Añadir el costo heurístico al final\n",
    "            while current_node:\n",
    "                path.append(current_node)\n",
    "                current_node = came_from.get(current_node)\n",
    "            return path[::-1], total_cost_with_heuristic  # Regresar el camino y el costo total con heurístico\n",
    "        \n",
    "        visited.add(current_node)\n",
    "        \n",
    "        for neighbor, cost in graph[current_node]:\n",
    "            new_cost = cost_so_far[current_node] + cost\n",
    "            if neighbor not in visited and (neighbor not in cost_so_far or new_cost < cost_so_far[neighbor]):\n",
    "                cost_so_far[neighbor] = new_cost\n",
    "                priority = new_cost + heuristic[neighbor]\n",
    "                heapq.heappush(frontier, (priority, neighbor))\n",
    "                came_from[neighbor] = current_node\n",
    "\n",
    "    return None, None  # No path found\n",
    "\n",
    "\n",
    "# Leer los archivos y obtener las estructuras\n",
    "graph = read_graph('Data/Grafo.txt')\n",
    "heuristic = read_heuristic('Data/heuristica.txt')\n",
    "\n",
    "# Encontrar el camino de 'A' a 'E' usando la búsqueda A*\n",
    "path = a_star_graph_search(graph, 'A', 'E', heuristic)\n",
    "print(\"A* Graph Search Path:\", path)\n",
    "\n"
   ],
   "id": "9682b8a601eb4faf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A* Graph Search Path: (['A', 'C', 'D', 'E'], 8)\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
